# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TPWnsXgEemfq9fBgQr7WR7l9PNKn_3iz
"""

import os
import gymnasium as gym
import numpy as np
import pandas as pd
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, CallbackList, CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold
from typing import List, Dict, Tuple, Optional

# Import môi trường giao dịch VN30 từ file env.py
from env import VN30TradingEnv  # Đảm bảo VN30TradingEnv đã được định nghĩa trong env.py

# Định nghĩa các chiến lược giao dịch từ trading_strategies.py
from trading_strategies import (
    GridStrategy,
    MomentumStrategy,
    MedianReversionStrategy,
    MarketArbitrageStrategy,
    MarketNeutralStrategy,
    PairTradingStrategy,
    EventDrivenStrategy,
    BetaStrategy,
    StatisticalArbitrageStrategy,
    ScalpingStrategy,
    ETFRebalanceStrategy,
    MarketMakingStrategy,
)

def train_ppo(
    env,
    eval_env=None,
    total_timesteps=10_000,
    checkpoint_freq=10_000,
    checkpoint_dir="./checkpoints",
    tensorboard_log="./tensorboard_logs",
    policy="MlpPolicy",
    reward_threshold=None,
    normalize=True,
    verbose=1,
    **ppo_kwargs
):
    os.makedirs(checkpoint_dir, exist_ok=True)
    os.makedirs(tensorboard_log, exist_ok=True)

    # Kiểm tra nếu môi trường chưa được đóng gói trong VecEnv
    if not hasattr(env, "num_envs"):
        env = Monitor(env, filename=os.path.join(checkpoint_dir, 'monitor.csv'))
        env = DummyVecEnv([lambda: env])
        if normalize:
            env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)
    else:
        if normalize:
            env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)
        env = VecMonitor(env)

    # Tạo callback để lưu checkpoint
    checkpoint_callback = CheckpointCallback(
        save_freq=checkpoint_freq,
        save_path=checkpoint_dir,
        name_prefix="ppo_checkpoint"
    )

    # Tạo callback dừng huấn luyện nếu đạt reward threshold
    stop_callback = None
    if reward_threshold is not None:
        stop_callback = StopTrainingOnRewardThreshold(
            reward_threshold=reward_threshold,
            verbose=1
        )

    # Tạo callback đánh giá nếu eval_env được cung cấp
    eval_callback = None
    if eval_env:
        if not hasattr(eval_env, "num_envs"):
            eval_env = Monitor(eval_env, filename=os.path.join(checkpoint_dir, 'eval_monitor.csv'))
            eval_env = DummyVecEnv([lambda: eval_env])
            if normalize:
                eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0)
        else:
            if normalize:
                eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.0)
            eval_env = VecMonitor(eval_env)

        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=checkpoint_dir,
            log_path=tensorboard_log,
            eval_freq=checkpoint_freq,
            deterministic=True,
            render=False,
            callback_after_eval=stop_callback
        )

    # Kết hợp tất cả các callback
    callback_list = [checkpoint_callback]
    if eval_callback:
        callback_list.append(eval_callback)

    callback = CallbackList(callback_list)

    # Khởi tạo mô hình PPO
    model = PPO(
        policy,
        env,
        verbose=verbose,
        tensorboard_log=tensorboard_log,
        **ppo_kwargs
    )

    # Bắt đầu huấn luyện
    model.learn(total_timesteps=total_timesteps, callback=callback)

    return model

# Đọc dữ liệu VN30
df = pd.read_csv("data/vn30_data.csv")  # Dữ liệu phải có các cột: ['date', 'id', 'open', 'high', 'low', 'close']

# Danh sách 30 mã VN30
tickers = [
    'ACB','BCM','BID','BVH','CTG','FPT','GAS','GVR','HDB','HPG',
    'KDH','MBB','MSN','MWG','NVL','PDR','PLX','POW','SAB','SHB',
    'SSB','SSI','STB','TCB','TPB','VCB','VHM','VIB','VIC','VNM'
]

# Bước 1: Tính toán hiệu suất các cặp cổ phiếu
def calculate_performance(df):
    # Tính toán lợi nhuận hàng ngày cho mỗi cổ phiếu
    df['daily_return'] = df['close'].pct_change()
    return df

# Tính hiệu suất cho tất cả các cổ phiếu trong danh sách tickers
performance_dict = {}
for ticker in tickers:
    df_ticker = df[df['id'] == ticker].reset_index(drop=True)
    df_ticker = calculate_performance(df_ticker)
    total_return = df_ticker['daily_return'].sum()
    performance_dict[ticker] = total_return

# Sắp xếp các cổ phiếu theo hiệu suất (total_return) và chọn top 5
top_5_tickers = sorted(performance_dict, key=performance_dict.get, reverse=True)[:5]
print("Top 5 cổ phiếu có hiệu suất tốt nhất:", top_5_tickers)

# Bước 2: Lấy dữ liệu cho các cặp cổ phiếu top 5
top_5_pairs = []
for i in range(len(top_5_tickers)):
    for j in range(i+1, len(top_5_tickers)):
        pair_df_1 = df[df['id'] == top_5_tickers[i]].reset_index(drop=True)
        pair_df_2 = df[df['id'] == top_5_tickers[j]].reset_index(drop=True)
        top_5_pairs.append((top_5_tickers[i], top_5_tickers[j], pair_df_1, pair_df_2))

# Bước 3: Cập nhật chiến lược giao dịch với các cặp cổ phiếu top 5
strategy_dict = {
    "Grid":              GridStrategy(grid_size=1.0, base_price=100.0),
    "Momentum":          MomentumStrategy(short_window=5, long_window=20),
    "MedianReversion":   MedianReversionStrategy(window=20),
    "MarketArbitrage":   [MarketArbitrageStrategy(pair_df_1, pair_df_2) for _, _, pair_df_1, pair_df_2 in top_5_pairs],
    "MarketNeutral":     MarketNeutralStrategy(),
    "PairTrading":       [PairTradingStrategy(pair_df_1, pair_df_2) for _, _, pair_df_1, pair_df_2 in top_5_pairs],
    "EventDriven":       EventDrivenStrategy(trigger_price=100.0),
    "Beta":              BetaStrategy(threshold=0.01),
    "StatArbitrage":     [StatisticalArbitrageStrategy(pair_df_1, pair_df_2) for _, _, pair_df_1, pair_df_2 in top_5_pairs],
    "Scalping":          ScalpingStrategy(window=1),
    "ETFRebalance":      ETFRebalanceStrategy(etf_weights={t: 1/len(tickers) for t in tickers}),
    "MarketMaking":      MarketMakingStrategy(spread=0.5),
}

# Khởi tạo môi trường giao dịch VN30
env = VN30TradingEnv(df=df, tickers=tickers, strategy_dict=strategy_dict)

# Môi trường đánh giá (nếu có)
eval_env = VN30TradingEnv(df=df, tickers=tickers, strategy_dict=strategy_dict)

# Huấn luyện PPO
trained_model = train_ppo(
    env,
    eval_env=eval_env,
    total_timesteps=10_000,
    checkpoint_freq=10_000,
    checkpoint_dir="./checkpoints",
    tensorboard_log="./tensorboard_logs",
    reward_threshold=1000000,  # Ví dụ về ngưỡng reward
    normalize=True
)

# Lưu mô hình đã huấn luyện
trained_model.save("ppo_vn30_trading_agent")